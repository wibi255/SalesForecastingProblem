# -*- coding: utf-8 -*-
"""Sales_LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PETH69jB0jrULmfo_W2-pB4ulfyejfIR
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random

np.random.seed(42)
random.seed(42)

start_date = datetime(2020, 1, 1)
end_date = datetime(2022, 12, 31)
dates = pd.date_range(start_date, end_date)

sales = []
stocks = []
for date in dates:
    base_sales = 60 + 8 * np.sin(2 * np.pi * date.timetuple().tm_yday / 365)
    weekday_effect = 15 if date.weekday() in [4, 5] else 0
    random_variation = np.random.normal(0, 1)
    temperature_effect = 1.5 if 3 <= date.month <= 9 else -1.5
    sales_today = max(0, int(base_sales + weekday_effect + random_variation + temperature_effect))
    stock_today = sales_today + random.randint(5, 15)
    sales.append(sales_today)
    stocks.append(stock_today)

promo_dates = []
promo_effects = []
for date in dates:
    if date.weekday() in [4, 5] or (3 <= date.month <= 5):
        promo_dates.append(date)
        promo_effects.append(random.choice([20, 30, 50]))

promotions = pd.DataFrame({
    'Tanggal': promo_dates,
    'Diskon (%)': promo_effects
})

promotion_effect = {10: 1.05, 20: 1.2, 30: 1.5, 50: 2.0}
sales_with_promo = []
for date, sale in zip(dates, sales):
    if date in promo_dates:
        discount = promotions.loc[promotions['Tanggal'] == date, 'Diskon (%)'].values[0]
        sale *= promotion_effect[discount]
    sales_with_promo.append(int(sale))

weather = pd.DataFrame({
    'Tanggal': dates,
    'Suhu (\u00b0C)': [round(np.random.normal(25, 5) if 3 <= d.month <= 9 else np.random.normal(18, 5), 1) for d in dates],
    'Curah Hujan (mm)': [max(0, int(np.random.normal(5, 10))) for _ in dates]
})

holiday_dates = random.sample(list(dates), k=15 * 3)
holidays = pd.DataFrame({
    'Tanggal': sorted(holiday_dates),
    'Hari Libur': ['Ya' for _ in holiday_dates]
})

sales_on_holidays = []
for date, sale in zip(dates, sales_with_promo):
    if date in holiday_dates:
        sale *= 1.5
    sales_on_holidays.append(int(sale))

sales_data = pd.DataFrame({
    'Tanggal': dates,
    'Jumlah Terjual': sales_on_holidays,
    'Jumlah Stok': stocks
})

window_size = 14
sales_data['Jumlah Terjual'] = sales_data['Jumlah Terjual'].rolling(window=window_size, min_periods=1).mean()
weather['Suhu (\u00b0C)'] = weather['Suhu (\u00b0C)'].rolling(window=window_size, min_periods=1).mean()
weather['Curah Hujan (mm)'] = weather['Curah Hujan (mm)'].rolling(window=window_size, min_periods=1).mean()

merged_data = sales_data.merge(weather, on='Tanggal', how='left')
merged_data = merged_data.merge(promotions, on='Tanggal', how='left')
merged_data = merged_data.merge(holidays, on='Tanggal', how='left')

merged_data['Diskon (%)'] = merged_data['Diskon (%)'].fillna(0)
merged_data['Hari Libur'] = merged_data['Hari Libur'].fillna('Tidak')

merged_data.to_csv("smoothed_sales_data.csv", index=False)
promotions.to_csv("promotions.csv", index=False)
weather.to_csv("weather.csv", index=False)
holidays.to_csv("holidays.csv", index=False)

print("Smoothed datasets generated and saved as CSV files:")
print("- smoothed_sales_data.csv")
print("- promotions.csv")
print("- weather.csv")
print("- holidays.csv")

merged_data['Hari Libur'] = merged_data['Hari Libur'].apply(lambda x: 1 if x == 'Ya' else 0)

merged_data.head()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


correlation_matrix = merged_data.corr()
print(correlation_matrix)


plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
plt.title('Correlation Matrix')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt


plt.figure(figsize=(14, 8))
plt.plot(merged_data['Tanggal'], merged_data['Jumlah Terjual'], label='Jumlah Terjual', color='blue', linewidth=2)
plt.plot(merged_data['Tanggal'], merged_data['Jumlah Stok'], label='Jumlah Stok', color='green', linewidth=2)

plt.plot(merged_data['Tanggal'], merged_data['Suhu (°C)'], label='Suhu (°C)', color='red', linewidth=2)
plt.plot(merged_data['Tanggal'], merged_data['Curah Hujan (mm)'], label='Curah Hujan (mm)', color='purple', linewidth=2)

plt.plot(merged_data['Tanggal'], merged_data['Diskon (%)'], label='Diskon (%)', color='orange', linewidth=2)

holidays = merged_data[merged_data['Hari Libur'] == 'Ya']
plt.scatter(holidays['Tanggal'], holidays['Jumlah Terjual'], label='Hari Libur', color='black', s=50, marker='x')

plt.title('Sales, Stock, Weather, Promotions, and Holidays Over Time')
plt.xlabel('Tanggal')
plt.ylabel('Values')
plt.legend(loc='upper left')
plt.xticks(rotation=45)
plt.grid(True)

plt.tight_layout()
plt.show()

merged_data.columns.to_list()

fitur = merged_data[['Suhu (°C)', 'Diskon (%)', 'Hari Libur']]
target = merged_data['Jumlah Terjual']

from sklearn.preprocessing import StandardScaler, PowerTransformer

# Standarisasi fitur menggunakan StandardScaler
scaler = StandardScaler()
fitur_scaled = scaler.fit_transform(fitur)

# Transformasi target menggunakan Yeo-Johnson
transformer = StandardScaler()
target_scaled = transformer.fit_transform(target.values.reshape(-1, 1))

# Hasil transformasi
print(f'Fitur setelah distandarisasi:\n{fitur_scaled[:5]}')  # Cek 5 data pertama
print(f'Target setelah Yeo-Johnson transformasi:\n{target_scaled[:5]}')  # Cek 5 data pertama

# Plotting the scaled features and transformed target
plt.figure(figsize=(14, 6))

# Plot the scaled features
plt.subplot(1, 2, 1)
plt.plot(fitur_scaled[:10], label='Scaled Features', color='orange')
plt.title('Transformed Features with StandardScaler')
plt.legend()

# Plot the transformed target variable
plt.subplot(1, 2, 2)
plt.plot(target_scaled[:10], label='Transformed Target', color='red')
plt.title('Target Transformation with Yeo-Johnson')
plt.legend()

# Display the plot
plt.tight_layout()
plt.show()

# Konversi hasil transformasi menjadi DataFrame
fitur_scaled = pd.DataFrame(fitur_scaled, columns=fitur.columns)
target_scaled = pd.DataFrame(target_scaled, columns=['Jumlah Terjual'])

# Gabungkan fitur dan target menjadi satu DataFrame
data_final_scaled = pd.concat([fitur_scaled, target_scaled], axis=1)

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split

def create_sequences(data, target, seq_length):
    X, y = [], []

    if isinstance(data, np.ndarray):
        data = pd.DataFrame(data)

    if isinstance(target, np.ndarray):
        target = pd.Series(target)

    target = target.reset_index(drop=True)

    for i in range(len(data) - seq_length):
        X.append(data.iloc[i:i + seq_length].values)
        y.append(target.iloc[i + seq_length])

    return np.array(X), np.array(y)

# Menentukan panjang urutan dinamis
dynamic_seq_length = 3  # Misalnya, panjang urutan yang diinginkan
# Gantilah `features_scaled` dan `target` dengan data Anda yang relevan
X, y = create_sequences(fitur_scaled, target_scaled, dynamic_seq_length)

print("Shape of X before padding:", X.shape)
print("Shape of y:", y.shape)

# Padding untuk menangani panjang input dinamis
X_padded = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=dynamic_seq_length, padding='post', dtype='float32')

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, shuffle=False)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)

# Ensure your data is in float32 format
X_train = X_train.astype('float32')
y_train = y_train.astype('float32')

pip install tensorflow-probability

pip install pandas numpy joblib sklearn tensorflow keras-tuner arch

pip install keras-tcn

pip install keras-tuner

import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import RobustScaler  # Import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import RandomizedSearchCV
from tensorflow.keras.callbacks import TensorBoard
import datetime
import matplotlib.pyplot as plt

import shutil
import os

# Nama direktori yang akan dihapus
directory = 'my_dir'

# Periksa apakah direktori ada
if os.path.exists(directory):
    # Hapus direktori dan semua isinya
    shutil.rmtree(directory)
    print(f'Direktori "{directory}" berhasil dihapus.')
else:
    print('File Tidak Ditemukan')

def build_model(hp):
    input_shape = (dynamic_seq_length, X.shape[2])
    inputs = Input(shape=input_shape)

    num_conv_layers = hp.Int('num_conv_layers', min_value=1, max_value=3)
    kernel_sizes = [hp.Choice(f'kernel_size_{i}', values=[6, 12, 24, 48]) for i in range(num_conv_layers)]
    filters = hp.Int('filters', min_value=16, max_value=128, step=16)
    dilation_rates = [hp.Choice(f'dilation_rate_{i}', values=[1, 2, 4]) for i in range(num_conv_layers)]
    activation_conv = hp.Choice('activation_conv', values=['swish', 'tanh', 'relu'])
    initial_dropout_rate = hp.Float('initial_dropout_rate', min_value=0.2, max_value=0.5, step=0.1)

    num_lstm_layers = hp.Int('num_lstm_layers', min_value=1, max_value=6)
    lstm_units = [hp.Int(f'lstm_units_{i}', min_value=30, max_value=150, step=10) for i in range(num_lstm_layers)]
    activation_lstm = hp.Choice('activation_lstm', values=['tanh', 'swish', 'relu'])

    max_norm = hp.Float('max_norm', min_value=1.0, max_value=5.0, step=0.5, default=3.0)

    l1_reg = hp.Float('l1_reg', min_value=0.0, max_value=0.1, step=0.01, default=0.001)
    l2_reg = hp.Float('l2_reg', min_value=0.0, max_value=0.1, step=0.01, default=0.001)

    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')
    gradient_clip_value = hp.Float('gradient_clip_value', min_value=1.0, max_value=5.0)
    weight_decay = hp.Float('weight_decay', min_value=1e-6, max_value=1e-2, sampling='log')

    alpha = hp.Float('alpha', min_value=0.1, max_value=10.0, step=0.1, default=2.0)
    beta = hp.Float('beta', min_value=0.1, max_value=5.0, step=0.1, default=1.0)

    additional_loss_weight = hp.Float('additional_loss_weight', min_value=0.0, max_value=1.0, step=0.1, default=0.1)
    sensitivity_adjustment = hp.Float('sensitivity_adjustment', min_value=0.5, max_value=2.0, step=0.1, default=1.0)
    outlier_loss_factor = hp.Float('outlier_loss_factor', min_value=0.1, max_value=1.0, step=0.05, default=0.3)
    trend_direction_loss_factor = hp.Float('trend_direction_loss_factor', min_value=0.1, max_value=1.0, step=0.05, default=0.5)
    reward_scaling = hp.Float('reward_scaling', min_value=0.1, max_value=2.0, step=0.1, default=1.0)
    direction_loss_weight = hp.Float('direction_loss_weight', min_value=0.1, max_value=2.0, step=0.1, default=1.0)
    asymmetric_loss_weight = hp.Float('asymmetric_loss_weight', min_value=0.1, max_value=2.0, step=0.1, default=1.0)
    reward_factor = hp.Float('reward_factor', min_value=0.1, max_value=2.0, step=0.1, default=1.0)

    conv_initializer = hp.Choice('conv_initializer', values=['glorot_uniform', 'he_normal'])
    lstm_initializer = hp.Choice('lstm_initializer', values=['glorot_uniform', 'orthogonal'])

    conv_output = inputs
    for i in range(num_conv_layers):
        conv_output = Conv1D(filters=filters, kernel_size=kernel_sizes[i],
                             dilation_rate=dilation_rates[i], padding='causal', activation=activation_conv,
                             kernel_initializer=conv_initializer)(conv_output)
        conv_output = Dropout(rate=initial_dropout_rate)(conv_output)

    lstm_output = conv_output
    for i in range(num_lstm_layers):
        lstm_output = Bidirectional(LSTM(units=lstm_units[i], return_sequences=True, activation=activation_lstm,
                                         kernel_regularizer=regularizers.L1L2(l1=l1_reg, l2=l2_reg),
                                         kernel_constraint=MaxNorm(max_norm),
                                         kernel_initializer=lstm_initializer))(lstm_output)
        lstm_output = Dropout(rate=initial_dropout_rate)(lstm_output)

    lstm_final_output = Bidirectional(LSTM(units=lstm_units[-1], return_sequences=False, activation=activation_lstm,
                                           kernel_regularizer=regularizers.L1L2(l1=l1_reg, l2=l2_reg),
                                           kernel_constraint=MaxNorm(max_norm),
                                           kernel_initializer=lstm_initializer))(lstm_output)
    lstm_final_output = LayerNormalization()(lstm_final_output)
    lstm_final_output = Dropout(rate=initial_dropout_rate)(lstm_final_output)

    predictions = Dense(1)(lstm_final_output)

    recent_predictions = tf.keras.layers.Lambda(lambda x: x[:, -3:, :])(inputs)

    combined = PredictiveCodingLayer()([predictions, recent_predictions])

    outputs = Dense(1)(combined[:, 0, :])

    optimizer_choice = hp.Choice('optimizer', values=['adam', 'adamw', 'nadam', 'sgd', 'rmsprop'])

    if optimizer_choice == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=gradient_clip_value)
    elif optimizer_choice == 'adamw':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, weight_decay=weight_decay, clipnorm=gradient_clip_value)
    elif optimizer_choice == 'nadam':
        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate, clipnorm=gradient_clip_value)
    elif optimizer_choice == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, clipnorm=gradient_clip_value)
    else:
        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, clipnorm=gradient_clip_value)

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=optimizer,
        loss=madal_loss(alpha=alpha, beta=beta, weight_decay=weight_decay,
                        additional_loss_weight=additional_loss_weight,
                        sensitivity_adjustment=sensitivity_adjustment,
                        outlier_loss_factor=outlier_loss_factor,
                        trend_direction_loss_factor=trend_direction_loss_factor,
                        reward_scaling=reward_scaling,
                        direction_loss_weight=direction_loss_weight,
                        asymmetric_loss_weight=asymmetric_loss_weight,
                        reward_factor=reward_factor),
        metrics=['mae', rmse, directional_accuracy]
    )

    return model

X_train = X_train.astype('float32')
y_train = y_train.astype('float32')

kf = KFold(n_splits=10, shuffle=False)

tuner = kt.BayesianOptimization(
    build_model,
    objective=kt.Objective('val_mae', direction='min'),
    max_trials=50,
    directory='my_dir',
    project_name='lstm_cnn_lstm'
)

dynamic_dropout_callback = DynamicDropoutCallback(0.2, 0.02, patience=3)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min', verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, mode='min', verbose=1)

for train_index, val_index in kf.split(X_train):
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    tuner.search(
        X_train_fold,
        y_train_fold,
        epochs=10,
        validation_data=(X_val_fold, y_val_fold),
        verbose=2,
        callbacks=[dynamic_dropout_callback]
    )

best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]

from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import KFold
import keras_tuner as kt

# Ensure your data is in float32 format
X_train = X_train.astype('float32')
y_train = y_train.astype('float32')

# K-Fold Cross-Validation without shuffling
kf = KFold(n_splits=3, shuffle=False)

# Keras Tuner setup
tuner = kt.BayesianOptimization(
    build_model,
    objective=kt.Objective('val_mae', direction='min'),
    max_trials=10,
    directory='my_dir',
    project_name='lstm_cnn_lstm'
)

# Callbacks
initial_dropout_rate = 0.2
dropout_increment = 0.02
dynamic_dropout_callback = DynamicDropoutCallback(initial_dropout_rate, dropout_increment, patience=3)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min', verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, mode='min', verbose=1)

# Initial trials with fewer epochs
for train_index, val_index in kf.split(X_train):
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Run initial trials
    tuner.search(
        X_train_fold,
        y_train_fold,
        epochs=10,  # Fewer epochs for initial trials
        validation_data=(X_val_fold, y_val_fold),
        verbose=2,
        callbacks=[dynamic_dropout_callback]
    )

# Retrieve best hyperparameters from initial trials
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]

# Mendapatkan model terbaik
best_model = tuner.get_best_models(num_models=1)[0]

# Evaluasi model terbaik
loss = best_model.evaluate(X_test, y_test)
print(f"Loss pada set pengujian: {loss}")

# Menyimpan model terbaik
best_model.save('ModelHargaH4.keras')

# Print the best hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best hyperparameters:")
for param, value in best_hyperparameters.values.items():
    print(f"{param}: {value}")

# Print the best model
best_model = tuner.get_best_models(num_models=1)[0]
print("\nBest model summary:")
best_model.summary()

import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import Callback

class MetricsPlotter(Callback):
    def __init__(self):
        super().__init__()
        self.history = {
            'loss': [], 'val_loss': [],
            'mae': [], 'val_mae': [],
            'rmse': [], 'val_rmse': [],
            'directional_accuracy': [], 'val_directional_accuracy': []
        }

    def on_epoch_end(self, epoch, logs=None):
        # Simpan nilai metrik pada setiap epoch
        for key in self.history.keys():
            if key in logs:
                self.history[key].append(logs[key])

    def on_train_end(self, logs=None):
        # Plot semua metrik evaluasi setelah training selesai
        self.plot_metrics()

    def plot_metrics(self):
        epochs = len(self.history['loss'])
        plt.figure(figsize=(16, 12))

        # Plot Loss
        plt.subplot(2, 2, 1)
        plt.plot(range(1, epochs + 1), self.history['loss'], label='Training Loss')
        plt.plot(range(1, epochs + 1), self.history['val_loss'], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Loss per Epoch')
        plt.legend()

        # Plot MAE
        plt.subplot(2, 2, 2)
        plt.plot(range(1, epochs + 1), self.history['mae'], label='Training MAE')
        plt.plot(range(1, epochs + 1), self.history['val_mae'], label='Validation MAE')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.title('Mean Absolute Error per Epoch')
        plt.legend()

        # Plot RMSE
        plt.subplot(2, 2, 3)
        plt.plot(range(1, epochs + 1), self.history['rmse'], label='Training RMSE')
        plt.plot(range(1, epochs + 1), self.history['val_rmse'], label='Validation RMSE')
        plt.xlabel('Epoch')
        plt.ylabel('RMSE')
        plt.title('Root Mean Squared Error per Epoch')
        plt.legend()

        # Plot Directional Accuracy
        plt.subplot(2, 2, 4)
        plt.plot(range(1, epochs + 1), self.history['directional_accuracy'], label='Training Directional Accuracy')
        plt.plot(range(1, epochs + 1), self.history['val_directional_accuracy'], label='Validation Directional Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Directional Accuracy')
        plt.title('Directional Accuracy per Epoch')
        plt.legend()

        plt.tight_layout()
        plt.show()

# Tambahkan callback ke proses training
metrics_plotter = MetricsPlotter()

from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Dropout

class DynamicDropoutCallback(Callback):
    def __init__(self, initial_rate, increment, patience=3, min_rate=0.1, max_rate=0.5, avg_window=5):
        super().__init__()
        self.dropout_rate = initial_rate
        self.increment = increment
        self.patience = patience
        self.wait = 0
        self.best_loss = float('inf')
        self.min_rate = min_rate
        self.max_rate = max_rate
        self.avg_window = avg_window
        self.losses = []

    def on_epoch_end(self, epoch, logs=None):
        current_loss = logs.get('val_loss')
        if current_loss is not None:
            # Store the loss to calculate the moving average
            self.losses.append(current_loss)
            if len(self.losses) > self.avg_window:
                self.losses.pop(0)  # Remove the oldest loss

            # Calculate the average loss
            avg_loss = sum(self.losses) / len(self.losses)

            if avg_loss < self.best_loss:
                self.best_loss = avg_loss
                self.wait = 0
            else:
                self.wait += 1

            # Adjust dropout rate
            if self.wait >= self.patience:
                # Increase dropout rate
                if self.dropout_rate < self.max_rate:
                    self.dropout_rate = min(self.dropout_rate + self.increment, self.max_rate)
                    self.wait = 0
                    self._update_dropout_rate()
                    print(f'Dropout rate increased to: {self.dropout_rate}')

            # Detect overfitting with rising average loss
            if len(self.losses) == self.avg_window and avg_loss > self.best_loss:
                if self.dropout_rate > self.min_rate:
                    self.dropout_rate = max(self.dropout_rate - self.increment, self.min_rate)
                    self._update_dropout_rate()
                    print(f'Dropout rate decreased to: {self.dropout_rate}')

    def _update_dropout_rate(self):
        for layer in self.model.layers:
            if isinstance(layer, Dropout):
                layer.rate = self.dropout_rate


# Retrain the best model with the entire dataset
history = best_model.fit(
    X_train,
    y_train,
    epochs=100,  # Adjust as needed for the number of epochs
    validation_split=0.2,  # Use 20% of the data for validation
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6),
        DynamicDropoutCallback(initial_rate=0.2, increment=0.05, patience=10),
        metrics_plotter# Ensure parameters are appropriate
    ],
    verbose=2  # Display training progress
)

import matplotlib.pyplot as plt

# Assuming `history` contains the training results
# Plotting the training and validation loss over epochs
plt.figure(figsize=(10, 6))

# Plot Training and Validation Loss
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='red')

# Adding labels and title
plt.title('Training and Validation Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Hitung metrik regresi
def calculate_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return mae, mse, rmse, r2

# Hitung akurasi arah dan rasio hit
def calculate_directional_accuracy(y_true, y_pred):
    directional_accuracy = np.mean(np.sign(y_pred[1:] - y_pred[:-1]) == np.sign(y_true[1:] - y_true[:-1]))
    hit_ratio = np.mean(np.sign(y_pred) == np.sign(y_true))
    return directional_accuracy, hit_ratio

# Fungsi untuk memplot prediksi vs nilai aktual
def plot_predictions(y_true, y_pred, title='Predictions vs Actual'):
    plt.figure(figsize=(14, 6))
    plt.plot(y_true, label='Actual', alpha=0.6)
    plt.plot(y_pred, label='Predicted', alpha=0.6)
    plt.title(title)
    plt.xlabel('Sample Index')
    plt.ylabel('Value')
    plt.legend()
    plt.show()

# Prediksi pada data latih dan uji
y_train_pred = best_model.predict(X_train).flatten()  # Flatten jika perlu
y_test_pred = best_model.predict(X_test).flatten()    # Flatten jika perlu

# Hitung metrik
train_mae, train_mse, train_rmse, train_r2 = calculate_metrics(y_train, y_train_pred)
test_mae, test_mse, test_rmse, test_r2 = calculate_metrics(y_test, y_test_pred)

# Hitung akurasi arah dan rasio hit
train_directional_accuracy, train_hit_ratio = calculate_directional_accuracy(y_train, y_train_pred)
test_directional_accuracy, test_hit_ratio = calculate_directional_accuracy(y_test, y_test_pred)

# Cetak hasil
print(f"Training Mean Absolute Error (MAE): {train_mae:.4f}")
print(f"Training Mean Squared Error (MSE): {train_mse:.4f}")
print(f"Training Root Mean Squared Error (RMSE): {train_rmse:.4f}")
print(f"Training R² Score: {train_r2:.4f}")
print(f"Training Directional Accuracy: {train_directional_accuracy:.4f}")
print(f"Training Hit Ratio: {train_hit_ratio:.4f}")

print(f"Testing Mean Absolute Error (MAE): {test_mae:.4f}")
print(f"Testing Mean Squared Error (MSE): {test_mse:.4f}")
print(f"Testing Root Mean Squared Error (RMSE): {test_rmse:.4f}")
print(f"Testing R² Score: {test_r2:.4f}")
print(f"Testing Directional Accuracy: {test_directional_accuracy:.4f}")
print(f"Testing Hit Ratio: {test_hit_ratio:.4f}")

# Plot prediksi vs nilai aktual untuk data latih dan uji
plot_predictions(y_train, y_train_pred, title='Training Predictions vs Actual')
plot_predictions(y_test, y_test_pred, title='Testing Predictions vs Actual')

# Fungsi untuk memplot learning curves
def plot_learning_curves(history):
    plt.figure(figsize=(14, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    if 'val_loss' in history.history:
        plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Learning Curves')
    plt.legend()
    plt.show()

# Panggil fungsi untuk memplot learning curves
plot_learning_curves(history)

# Visualisasi Arah Akurasi dan Rasio Hit
def plot_directional_accuracy_and_hit_ratio(train_acc, train_hit, test_acc, test_hit):
    labels = ['Training Directional Accuracy', 'Training Hit Ratio', 'Testing Directional Accuracy', 'Testing Hit Ratio']
    values = [train_acc, train_hit, test_acc, test_hit]

    plt.figure(figsize=(10, 6))
    plt.bar(labels, values, color=['blue', 'orange', 'green', 'red'])
    plt.ylim(0, 1)  # Set batas sumbu y dari 0 hingga 1
    plt.title('Directional Accuracy and Hit Ratio')
    plt.ylabel('Value')
    plt.grid(axis='y')
    plt.show()

# Panggil fungsi untuk memplot metrik
plot_directional_accuracy_and_hit_ratio(train_directional_accuracy, train_hit_ratio, test_directional_accuracy, test_hit_ratio)